{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9749279,"sourceType":"datasetVersion","datasetId":5968712},{"sourceId":9903298,"sourceType":"datasetVersion","datasetId":6083874},{"sourceId":9910332,"sourceType":"datasetVersion","datasetId":6089199},{"sourceId":9915208,"sourceType":"datasetVersion","datasetId":6092942},{"sourceId":9917477,"sourceType":"datasetVersion","datasetId":6094691},{"sourceId":9924321,"sourceType":"datasetVersion","datasetId":6099708},{"sourceId":9932712,"sourceType":"datasetVersion","datasetId":6105843},{"sourceId":9937308,"sourceType":"datasetVersion","datasetId":6109216}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\n!pip install sentence_transformers\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load preprocessed corpus\ndf = pd.read_csv('/kaggle/input/bkai-ai-track2-legal-document-retrieval/Legal Document Retrieval/public_test.csv')\n\n# Initialize the model\nmodel = SentenceTransformer('/kaggle/input/bkaibiencoderfinetuned/transformers/default/1/final')\nmodel = model.to(device)\n\ndef encode(lst = [], convert_to_tensor=True, batch_size=1024):\n    vectors = []\n    # Create progress bar\n    with tqdm(total=len(lst), desc=\"Encoding texts\") as pbar:\n        # Process in batches\n        for i in range(0, len(lst), batch_size):\n            batch = lst[i:i + batch_size]\n            encoded_batch = model.encode(batch, convert_to_tensor=True, device=device)\n            # Move to CPU before converting to numpy\n            if torch.cuda.is_available():\n                encoded_batch = encoded_batch.cpu()\n            vectors.extend([np.array(arr) for arr in encoded_batch.numpy()])\n            pbar.update(len(batch))\n    return vectors\n\n# Encode the text column\ndf['vector'] = encode(lst=list(df['question']))\n\n# Save the encoded corpus\ndf.to_json('encoded_public_test_finetuned.json')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_json('/kaggle/input/finetunedpublictest/encoded_public_test_finetuned.json')\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T01:43:39.687690Z","iopub.execute_input":"2024-11-21T01:43:39.688061Z","iopub.status.idle":"2024-11-21T01:43:41.387504Z","shell.execute_reply.started":"2024-11-21T01:43:39.688031Z","shell.execute_reply":"2024-11-21T01:43:41.386557Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                               question     qid  \\\n0     Hiệp hội Công nghiệp ghi âm Việt Nam hoạt động...   98440   \n1     Báo cáo nghiên cứu khả thi đầu tư xây dựng là ...  105737   \n2     Lịch khai giảng năm học 2022 - 2023 đối với họ...  106239   \n3     Số định danh cá nhân có được dùng thay thế các...   79491   \n4     Trợ cấp đối với Chủ tịch Hội cựu chiến binh cấ...  130557   \n...                                                 ...     ...   \n9995  Đón trả hành khách trên đường cao tốc có bị gi...   42798   \n9996  Các đơn vị được giao là đầu mối trao đổi, cung...   10533   \n9997  Ban Thường vụ Hội Hỗ trợ khắc phục hậu quả bom...   46794   \n9998  Tài liệu thông tin, giáo dục, truyền thông về ...  112007   \n9999  Quyền, nghĩa vụ của chiến đấu viên được quy đị...   14114   \n\n                                                 vector  \n0     [-0.0347496048, 0.25700578090000004, 0.3575423...  \n1     [0.1764316112, 0.16144044700000001, 0.07395143...  \n2     [0.0155817755, -0.11466823520000001, -0.470231...  \n3     [-0.048475273000000006, 0.3610271513, -0.23998...  \n4     [-0.0451854542, -0.06843171270000001, -0.28169...  \n...                                                 ...  \n9995  [-0.1179260835, -0.0827077851, -0.108169354500...  \n9996  [0.1132282391, -0.0720719025, -0.1837236732000...  \n9997  [-0.1014185771, -0.0322549231, -0.1336619556, ...  \n9998  [0.0728569478, -0.5358711481, 0.2039459348, -0...  \n9999  [0.07675954700000001, 0.24089975660000001, 0.0...  \n\n[10000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>qid</th>\n      <th>vector</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hiệp hội Công nghiệp ghi âm Việt Nam hoạt động...</td>\n      <td>98440</td>\n      <td>[-0.0347496048, 0.25700578090000004, 0.3575423...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Báo cáo nghiên cứu khả thi đầu tư xây dựng là ...</td>\n      <td>105737</td>\n      <td>[0.1764316112, 0.16144044700000001, 0.07395143...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Lịch khai giảng năm học 2022 - 2023 đối với họ...</td>\n      <td>106239</td>\n      <td>[0.0155817755, -0.11466823520000001, -0.470231...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Số định danh cá nhân có được dùng thay thế các...</td>\n      <td>79491</td>\n      <td>[-0.048475273000000006, 0.3610271513, -0.23998...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Trợ cấp đối với Chủ tịch Hội cựu chiến binh cấ...</td>\n      <td>130557</td>\n      <td>[-0.0451854542, -0.06843171270000001, -0.28169...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>Đón trả hành khách trên đường cao tốc có bị gi...</td>\n      <td>42798</td>\n      <td>[-0.1179260835, -0.0827077851, -0.108169354500...</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>Các đơn vị được giao là đầu mối trao đổi, cung...</td>\n      <td>10533</td>\n      <td>[0.1132282391, -0.0720719025, -0.1837236732000...</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>Ban Thường vụ Hội Hỗ trợ khắc phục hậu quả bom...</td>\n      <td>46794</td>\n      <td>[-0.1014185771, -0.0322549231, -0.1336619556, ...</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>Tài liệu thông tin, giáo dục, truyền thông về ...</td>\n      <td>112007</td>\n      <td>[0.0728569478, -0.5358711481, 0.2039459348, -0...</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>Quyền, nghĩa vụ của chiến đấu viên được quy đị...</td>\n      <td>14114</td>\n      <td>[0.07675954700000001, 0.24089975660000001, 0.0...</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\n!pip install sentence_transformers\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load data\ndf = pd.read_csv('/kaggle/input/bkai-ai-track2-legal-document-retrieval/Legal Document Retrieval/train.csv')\n\n# Split into train and test sets\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Initialize the model\nmodel = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder')\nmodel = model.to(device)\n\ndef encode(lst = [], convert_to_tensor=True, batch_size=512):\n    vectors = []\n    with tqdm(total=len(lst), desc=\"Encoding questions\") as pbar:\n        for i in range(0, len(lst), batch_size):\n            batch = lst[i:i + batch_size]\n            encoded_batch = model.encode(batch, convert_to_tensor=True, device=device)\n            if torch.cuda.is_available():\n                encoded_batch = encoded_batch.cpu()\n            vectors.extend([np.array(arr) for arr in encoded_batch.numpy()])\n            pbar.update(len(batch))\n    return vectors\n\n# Encode questions for both train and test sets\ntrain_df['question_vector'] = encode(lst=list(train_df['question']))\ntest_df['question_vector'] = encode(lst=list(test_df['question']))\n\n# Save encoded datasets\ntrain_df.to_json('encoded_train.json')\ntest_df.to_json('encoded_test.json')\n\nprint(f\"Training set size: {len(train_df)}\")\nprint(f\"Test set size: {len(test_df)}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T00:46:44.887741Z","iopub.execute_input":"2024-11-15T00:46:44.888045Z","iopub.status.idle":"2024-11-15T00:51:26.441984Z","shell.execute_reply.started":"2024-11-15T00:46:44.888011Z","shell.execute_reply":"2024-11-15T00:51:26.440984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(path, download_file_name):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip {zip_name} {path} -r\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n\ndownload_file('/kaggle/working', 'encoded_public_test_finetuned')\n# download_file('/kaggle/working/encoded_test.json', 'out')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T01:01:09.543417Z","iopub.execute_input":"2024-11-21T01:01:09.544206Z","iopub.status.idle":"2024-11-21T01:01:19.869069Z","shell.execute_reply.started":"2024-11-21T01:01:09.544170Z","shell.execute_reply":"2024-11-21T01:01:19.868193Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/encoded_public_test_finetuned.zip","text/html":"<a href='encoded_public_test_finetuned.zip' target='_blank'>encoded_public_test_finetuned.zip</a><br>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\n!pip install sentence_transformers\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\n# Load data with both question and qid columns\ndf = pd.read_csv('/kaggle/input/bkai-ai-track2-legal-document-retrieval/Legal Document Retrieval/corpus.csv')\n\n# Determine the device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Initialize the model with the specified device \nmodel = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder', device=device)\n\ndef encode(lst = [], convert_to_tensor=True, batch_size=128):\n    vectors = []\n    with tqdm(total=len(lst), desc=\"Encoding questions\") as pbar:\n        for i in range(0, len(lst), batch_size):\n            batch = lst[i:i + batch_size]\n            encoded_batch = model.encode(batch, convert_to_tensor=True)\n            if device == 'cuda':\n                encoded_batch = encoded_batch.cpu()\n            vectors.extend([np.array(arr) for arr in encoded_batch.numpy()])\n            pbar.update(len(batch))\n    return vectors\n\n# Encode questions while preserving original columns\ndf['question_vector'] = encode(lst=list(df['text']))\n\n# Select only the required columns\noutput_df = df[['text', 'qid', 'question_vector']]\n\n# Save to JSON\noutput_df.to_json('encoded_public_tes.json')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T14:12:34.714004Z","iopub.execute_input":"2024-11-16T14:12:34.714985Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport torch\n\n# Load encoded vectors from JSON files\nwith open('/kaggle/input/finetunedpublictest/encoded_public_test_finetuned.json', 'r') as f:\n    train_data = json.load(f)\n\nwith open('/kaggle/input/finetunedcorpus/encoded_corpus.json', 'r') as f:\n    corpus_data = json.load(f)\n\n# Extract vectors and IDs\ntrain_vectors = [item['vector'] for item in train_data]\ntrain_ids = [item['qid'] for item in train_data]\n\ncorpus_vectors = [item['vector'] for item in corpus_data]\ncorpus_ids = [item['cid'] for item in corpus_data]\n\n# Convert to PyTorch tensors and move to CUDA if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntrain_tensor = torch.tensor(train_vectors, device=device)\ncorpus_tensor = torch.tensor(corpus_vectors, device=device)\n\n# Normalize vectors\ntrain_tensor = torch.nn.functional.normalize(train_tensor, p=2, dim=1)\ncorpus_tensor = torch.nn.functional.normalize(corpus_tensor, p=2, dim=1)\n\n# Compute cosine similarity\nsimilarity_matrix = torch.matmul(train_tensor, corpus_tensor.T)\n\n# Get top K similar documents for each query\ntop_k = 50  # Adjust as needed\ntop_k_values, top_k_indices = torch.topk(similarity_matrix, k=top_k, dim=1)\n\n# Write results directly to text file in required format\nwith open('predict_top50.txt', 'w') as f:\n    for i, (values, indices) in enumerate(zip(top_k_values, top_k_indices)):\n        qid = train_ids[i]\n        # Convert indices to cids and create space-separated string\n        top_cids = [str(corpus_ids[idx]) for idx in indices.cpu().numpy()]\n        line = f\"{qid} {' '.join(top_cids)}\\n\"\n        f.write(line)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T01:46:08.597656Z","iopub.execute_input":"2024-11-21T01:46:08.598028Z","iopub.status.idle":"2024-11-21T01:46:59.255575Z","shell.execute_reply.started":"2024-11-21T01:46:08.597999Z","shell.execute_reply":"2024-11-21T01:46:59.254442Z"}},"outputs":[{"name":"stderr","text":"Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7e5d48793e20>>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n    def _clean_thread_parent_frames(\nKeyboardInterrupt: \nException ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7e5d48793e20>>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n    def _clean_thread_parent_frames(\nKeyboardInterrupt: \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     corpus_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Extract vectors and IDs\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m train_vectors \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m train_data]\n\u001b[1;32m     13\u001b[0m train_ids \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m train_data]\n\u001b[1;32m     15\u001b[0m corpus_vectors \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m corpus_data]\n","Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m     corpus_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Extract vectors and IDs\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m train_vectors \u001b[38;5;241m=\u001b[39m [\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvector\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m train_data]\n\u001b[1;32m     13\u001b[0m train_ids \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m train_data]\n\u001b[1;32m     15\u001b[0m corpus_vectors \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m corpus_data]\n","\u001b[0;31mTypeError\u001b[0m: string indices must be integers"],"ename":"TypeError","evalue":"string indices must be integers","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport pandas as pd\nimport zipfile\n\n# ...existing code...\n\n# Load JSON files using Pandas\ntrain_df = pd.read_json('/kaggle/input/encoded-public-test/encoded_public_test.json')\n# train_df = train_df.head(1000)\ncorpus_df = pd.read_json('/kaggle/input/encoded/encoded_corpus.json')\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n\n# Extract IDs and vectors\ntrain_ids = train_df['qid'].tolist()\ntrain_vectors = torch.tensor(train_df['question_vector'].tolist(), dtype=torch.float32).to(device)\ncorpus_ids = corpus_df['cid'].tolist()\ncorpus_vectors = torch.tensor(corpus_df['vector'].tolist(), dtype=torch.float32).to(device)\n\n# Normalize vectors\ntrain_vectors = F.normalize(train_vectors, p=2, dim=1)\ncorpus_vectors = F.normalize(corpus_vectors, p=2, dim=1)\n\n# Compute cosine similarity using PyTorch on CUDA\nsimilarity_matrix = torch.matmul(train_vectors, corpus_vectors.T)\n\n# Get top K similar documents\ntop_k = 50  # Adjust as needed\ntop_k_values, top_k_indices = torch.topk(similarity_matrix, k=top_k, dim=1)\n\n# Write results to predict.txt in required format\nwith open('predict_top50.txt', 'w') as f:\n    for i, indices in enumerate(top_k_indices):\n        qid = train_ids[i]\n        top_cids = [str(corpus_ids[idx.item()]) for idx in indices]\n        line = f\"{qid} {' '.join(top_cids)}\\n\"\n        f.write(line)\n\n# Zip the predict.txt file\nwith zipfile.ZipFile('predict_top50.zip', 'w') as zipf:\n    zipf.write('predict_top50.txt')\n\n# ...existing code...\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T14:02:29.513516Z","iopub.execute_input":"2024-11-16T14:02:29.513892Z"},"jupyter":{"outputs_hidden":true,"source_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\n!pip install sentence_transformers\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load preprocessed corpus\ndf = pd.read_csv('/kaggle/input/bkai-ai-track2-legal-document-retrieval/Legal Document Retrieval/train.csv')\n\n# Initialize the model\nmodel = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder')\nmodel = model.to(device)\n\ndef encode(lst = [], convert_to_tensor=True, batch_size=1024):\n    vectors = []\n    # Create progress bar\n    with tqdm(total=len(lst), desc=\"Encoding texts\") as pbar:\n        # Process in batches\n        for i in range(0, len(lst), batch_size):\n            batch = lst[i:i + batch_size]\n            encoded_batch = model.encode(batch, convert_to_tensor=True, device=device)\n            # Move to CPU before converting to numpy\n            if torch.cuda.is_available():\n                encoded_batch = encoded_batch.cpu()\n            vectors.extend([np.array(arr) for arr in encoded_batch.numpy()])\n            pbar.update(len(batch))\n    return vectors\n\n# Encode the text column\ndf['question_vector'] = encode(lst=list(df['question']))\n\n# Save the encoded corpus\ndf.to_json('encoded_train_full.json')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T15:41:33.222066Z","iopub.execute_input":"2024-11-15T15:41:33.223075Z","iopub.status.idle":"2024-11-15T15:44:07.175259Z","shell.execute_reply.started":"2024-11-15T15:41:33.223031Z","shell.execute_reply":"2024-11-15T15:44:07.174103Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForMaskedLM,\n    DataCollatorForLanguageModeling,\n    get_scheduler,\n    default_data_collator\n)\nfrom accelerate import Accelerator\nfrom torch.optim import AdamW\nfrom tqdm.auto import tqdm\nimport math\nimport os\nimport json\n\nclass VietnameseDataset(Dataset):\n    def __init__(self, csv_path, tokenizer, max_length=256):\n        self.df = pd.read_csv(csv_path)\n        self.texts = self.df['text'].tolist()\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n        # Pre-tokenize all texts\n        self.encodings = self.tokenizer(\n            self.texts,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=None  # Changed this to return lists instead of tensors\n        )\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        # Return dictionary of items for the specified index\n        return {\n            key: torch.tensor(val[idx]) \n            for key, val in self.encodings.items()\n        }\n\nclass BERTMaskingTrainer:\n    def __init__(self, model_name=\"bkai-foundation-models/vietnamese-bi-encoder\", \n                 output_dir=\"model_checkpoints\", max_length=256):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForMaskedLM.from_pretrained(model_name)\n        self.output_dir = output_dir\n        self.max_length = max_length\n        self.mask_probability = 0.20  # 20% masking as specified\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Initialize accelerator\n        self.accelerator = Accelerator()\n        \n        # Initialize data collator for masked language modeling\n        self.data_collator = DataCollatorForLanguageModeling(\n            tokenizer=self.tokenizer,\n            mlm_probability=self.mask_probability\n        )\n        \n        # Add special tokens if needed\n        special_tokens = {'pad_token': '[PAD]', 'mask_token': '[MASK]'}\n        self.tokenizer.add_special_tokens(special_tokens)\n        self.model.resize_token_embeddings(len(self.tokenizer))\n\n    def prepare_data(self, csv_path, batch_size=16, test_size=0.1):  # Reduced batch size\n        dataset = VietnameseDataset(csv_path, self.tokenizer, self.max_length)\n        \n        # Split into train/test\n        train_size = int((1 - test_size) * len(dataset))\n        test_size = len(dataset) - train_size\n        train_dataset, test_dataset = torch.utils.data.random_split(\n            dataset, [train_size, test_size]\n        )\n\n        self.train_loader = DataLoader(\n            train_dataset,\n            batch_size=batch_size,\n            shuffle=True,\n            collate_fn=self.data_collator\n        )\n        \n        self.eval_loader = DataLoader(\n            test_dataset,\n            batch_size=batch_size,\n            collate_fn=default_data_collator\n        )\n\n    def save_checkpoint(self, epoch, loss):\n        checkpoint_dir = os.path.join(self.output_dir, f\"checkpoint-{epoch}\")\n        os.makedirs(checkpoint_dir, exist_ok=True)\n        \n        unwrapped_model = self.accelerator.unwrap_model(self.model)\n        unwrapped_model.save_pretrained(checkpoint_dir)\n        self.tokenizer.save_pretrained(checkpoint_dir)\n        \n        # Save training info\n        with open(os.path.join(checkpoint_dir, \"training_info.json\"), \"w\") as f:\n            json.dump({\"epoch\": epoch, \"loss\": loss}, f)\n\n    def train(self, num_epochs=30, learning_rate=5e-5):\n        # Prepare for training\n        self.model, self.optimizer, self.train_loader, self.eval_loader = \\\n            self.accelerator.prepare(\n                self.model, \n                AdamW(self.model.parameters(), lr=learning_rate),\n                self.train_loader, \n                self.eval_loader\n            )\n\n        num_training_steps = num_epochs * len(self.train_loader)\n        lr_scheduler = get_scheduler(\n            \"linear\",\n            optimizer=self.optimizer,\n            num_warmup_steps=0,\n            num_training_steps=num_training_steps\n        )\n\n        progress_bar = tqdm(range(num_training_steps))\n\n        for epoch in range(num_epochs):\n            # Training phase\n            self.model.train()\n            for batch in self.train_loader:\n                outputs = self.model(**batch)\n                loss = outputs.loss\n                self.accelerator.backward(loss)\n                \n                self.optimizer.step()\n                lr_scheduler.step()\n                self.optimizer.zero_grad()\n                progress_bar.update(1)\n\n            # Evaluation phase\n            self.model.eval()\n            eval_losses = []\n            for batch in self.eval_loader:\n                with torch.no_grad():\n                    outputs = self.model(**batch)\n                eval_losses.append(self.accelerator.gather(outputs.loss))\n\n            eval_loss = torch.mean(torch.cat(eval_losses))\n            try:\n                perplexity = math.exp(eval_loss)\n            except OverflowError:\n                perplexity = float(\"inf\")\n            \n            print(f\"Epoch {epoch+1}: Perplexity: {perplexity}\")\n            \n            # Save checkpoint\n            self.save_checkpoint(epoch+1, eval_loss.item())\n\nif __name__ == \"__main__\":\n    trainer = BERTMaskingTrainer()\n    trainer.prepare_data(\"/kaggle/input/preprocessed-corpus/preprocessed_corpus.csv\")\n    trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T01:25:01.466608Z","iopub.execute_input":"2024-11-16T01:25:01.467881Z","iopub.status.idle":"2024-11-16T03:05:51.605277Z","shell.execute_reply.started":"2024-11-16T01:25:01.467815Z","shell.execute_reply":"2024-11-16T03:05:51.602828Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling,\n                          get_scheduler, AdamW)\nfrom accelerate import Accelerator\nfrom tqdm.auto import tqdm\nimport math\n\nclass CustomTextDataset(Dataset):\n    def __init__(self, texts, tokenizer, block_size=128):\n        self.tokenizer = tokenizer\n        # Tokenize the texts\n        tokenized_inputs = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\n        self.input_ids = tokenized_inputs['input_ids']\n        self.attention_masks = tokenized_inputs['attention_mask']\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.input_ids[idx],\n            'attention_mask': self.attention_masks[idx]\n        }\n\nclass MaskedLanguageModelTrainer:\n    def __init__(self, csv_file, model_name, masking_prob=0.2, batch_size=32, num_epochs=30):\n        self.csv_file = csv_file\n        self.model_name = model_name\n        self.masking_prob = masking_prob\n        self.batch_size = batch_size\n        self.num_epochs = num_epochs\n\n        # Load tokenizer and model\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.model = AutoModelForMaskedLM.from_pretrained(self.model_name)\n\n        self.accelerator = Accelerator()\n        self.data_collator = DataCollatorForLanguageModeling(\n            tokenizer=self.tokenizer, mlm=True, mlm_probability=self.masking_prob\n        )\n\n    def load_data(self):\n        df = pd.read_csv(self.csv_file)\n        texts = df['text'].tolist()\n        self.dataset = CustomTextDataset(texts, self.tokenizer)\n\n    def prepare_dataloader(self):\n        self.dataloader = DataLoader(\n            self.dataset, batch_size=self.batch_size, shuffle=True, collate_fn=self.data_collator\n        )\n\n    def train(self):\n        optimizer = AdamW(self.model.parameters(), lr=5e-5)\n        self.model, optimizer, self.dataloader = self.accelerator.prepare(\n            self.model, optimizer, self.dataloader\n        )\n\n        num_update_steps_per_epoch = len(self.dataloader)\n        num_training_steps = self.num_epochs * num_update_steps_per_epoch\n        lr_scheduler = get_scheduler(\n            \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n        )\n\n        progress_bar = tqdm(range(num_training_steps))\n\n        for epoch in range(self.num_epochs):\n            self.model.train()\n            for batch in self.dataloader:\n                outputs = self.model(**batch)\n                loss = outputs.loss\n                self.accelerator.backward(loss)\n\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                progress_bar.update(1)\n\n            # Optional: Evaluate the model at the end of each epoch\n            perplexity = self.evaluate()\n            print(f\">>> Epoch {epoch + 1}: Perplexity: {perplexity}\")\n\n    def evaluate(self):\n        self.model.eval()\n        losses = []\n        for batch in self.dataloader:\n            with torch.no_grad():\n                outputs = self.model(**batch)\n            loss = outputs.loss\n            losses.append(self.accelerator.gather(loss.repeat(self.batch_size)))\n\n        losses = torch.cat(losses)\n        losses = losses[: len(self.dataset)]\n        try:\n            perplexity = math.exp(torch.mean(losses))\n        except OverflowError:\n            perplexity = float(\"inf\")\n        return perplexity\n\n    def save_model(self, output_dir):\n        self.accelerator.wait_for_everyone()\n        unwrapped_model = self.accelerator.unwrap_model(self.model)\n        unwrapped_model.save_pretrained(output_dir)\n        self.tokenizer.save_pretrained(output_dir)\n\nif __name__ == \"__main__\":\n    trainer = MaskedLanguageModelTrainer(\n        csv_file=\"/kaggle/input/preprocessed-corpus/preprocessed_corpus.csv\",\n        model_name=\"bkai-foundation-models/vietnamese-bi-encoder\"\n    )\n    trainer.load_data()\n    trainer.prepare_dataloader()\n    trainer.train()\n    trainer.save_model(output_dir=\"trained_model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T03:06:51.424203Z","iopub.execute_input":"2024-11-16T03:06:51.424552Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip3 install \n!ls -la /usr/lib/jvm/\n\nimport py_vncorenlp\npy_vncorenlp.download_model(save_dir='/kaggle/working')\nrdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir='/kaggle/working')\n\nquery = \"Trường UIT là gì?\"\nsentences = [\n    \"Trường Đại học Công nghệ Thông tin có tên tiếng Anh là University of Information Technology (viết tắt là UIT) là thành viên của Đại học Quốc Gia TP.HCM.\",\n    \"Trường Đại học Kinh tế – Luật (tiếng Anh: University of Economics and Law – UEL) là trường đại học đào tạo và nghiên cứu khối ngành kinh tế, kinh doanh và luật hàng đầu Việt Nam.\",\n    \"Quĩ uỷ thác đầu tư (tiếng Anh: Unit Investment Trusts; viết tắt: UIT) là một công ty đầu tư mua hoặc nắm giữ một danh mục đầu tư cố định\"\n]\n\ntokenized_query = rdrsegmenter.word_segment(query)\ntokenized_sentences = [rdrsegmenter.word_segment(sent) for sent in sentences]\n\ntokenized_pairs = [[tokenized_query, sent] for sent in tokenized_sentences]\n\nMODEL_ID = 'itdainb/PhoRanker'\nMAX_LENGTH = 256\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T13:43:13.053584Z","iopub.execute_input":"2024-11-16T13:43:13.054365Z","iopub.status.idle":"2024-11-16T13:43:14.499736Z","shell.execute_reply.started":"2024-11-16T13:43:13.054312Z","shell.execute_reply":"2024-11-16T13:43:14.498083Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is the code for cosine similarity and json output for topk","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport pandas as pd\nimport zipfile\nimport json\n\n# ...existing code...\n\n# Load JSON files using Pandas\ntrain_df = pd.read_json('/kaggle/input/finetunedpublictest/encoded_public_test_finetuned.json')\n# train_df = train_df.head(1000)\ncorpus_df = pd.read_json('/kaggle/input/finetunedcorpus/encoded_corpus.json')\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n\n# Extract IDs and vectors\ntrain_ids = train_df['qid'].tolist()\ntrain_vectors = torch.tensor(train_df['vector'].tolist(), dtype=torch.float32).to(device)\ncorpus_ids = corpus_df['cid'].tolist()\ncorpus_vectors = torch.tensor(corpus_df['vector'].tolist(), dtype=torch.float32).to(device)\n\n# Normalize vectors\ntrain_vectors = F.normalize(train_vectors, p=2, dim=1)\ncorpus_vectors = F.normalize(corpus_vectors, p=2, dim=1)\n\n# Compute cosine similarity using PyTorch on CUDA\nsimilarity_matrix = torch.matmul(train_vectors, corpus_vectors.T)\n\n# Get top K similar documents\ntop_k = 50  # Adjust as needed\ntop_k_values, top_k_indices = torch.topk(similarity_matrix, k=top_k, dim=1)\n\n# Create results for both TXT and JSON formats\njson_results = []\n\nwith open('predict_top50.txt', 'w') as f:\n    for i, (indices, scores) in enumerate(zip(top_k_indices, top_k_values)):\n        qid = train_ids[i]\n        \n        # Convert indices and scores to Python lists\n        top_cids = [str(corpus_ids[idx.item()]) for idx in indices]\n        similarity_scores = [score.item() for score in scores]\n        \n        # Write to TXT format\n        line = f\"{qid} {' '.join(top_cids)}\\n\"\n        f.write(line)\n        \n        # Prepare JSON format\n        # Format suitable for re-ranker: including query_id, candidate_ids, and their scores\n        json_entry = {\n            \"query_id\": qid,\n            \"candidates\": {\n                \"doc_ids\": top_cids,\n                \"scores\": similarity_scores,\n                # Additional fields that might be useful for re-ranker:\n            }\n        }\n        json_results.append(json_entry)\n\n# Save JSON results\nwith open('predict_top50.json', 'w', encoding='utf-8') as f:\n    json.dump(json_results, f, ensure_ascii=False, indent=2)\n\n# Zip both files\nwith zipfile.ZipFile('predict_top50.zip', 'w') as zipf:\n    zipf.write('predict_top50.txt')\n    zipf.write('predict_top50.json')\n\n# ...existing code...\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T01:50:04.158321Z","iopub.execute_input":"2024-11-21T01:50:04.158615Z","iopub.status.idle":"2024-11-21T01:51:25.464029Z","shell.execute_reply.started":"2024-11-21T01:50:04.158585Z","shell.execute_reply":"2024-11-21T01:51:25.463296Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport json\n\n# Load all required data\npredictions = pd.read_json('/kaggle/input/top50cosinejson/predict_top50.json')\ntest_df = pd.read_csv('/kaggle/input/bkai-ai-track2-legal-document-retrieval/Legal Document Retrieval/public_test.csv')\ncorpus_df = pd.read_csv('/kaggle/input/preprocessed-corpus/preprocessed_corpus.csv')\n\n# Fix: Create corpus dictionary with correct column mapping\ncorpus_dict = dict(zip(corpus_df['cid'], corpus_df['text']))\n\n# Setup model and device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntokenizer = AutoTokenizer.from_pretrained('namdp-ptit/ViRanker')\nmodel = AutoModelForSequenceClassification.from_pretrained('namdp-ptit/ViRanker').to(device)\nmodel.eval()\n\nreranked_results = []\n\n# Process each query\nfor _, row in predictions.iterrows():\n    # qid = row['query_id']\n    # # Fix: Use 'question' column instead of 'query'\n    # query_text = test_df[test_df['qid'] == qid]['question'].iloc[0]\n    # doc_ids = row['candidates']['doc_ids']\n    \n    # # Fix: Use corpus_dict correctly - get text for each doc_id\n    # pairs = [[query_text, corpus_dict[doc_id]] for doc_id in doc_ids]\n    \n    # Re-rank in batches\n    batch_size = 8  # Smaller batch size due to longer texts\n    all_scores = []\n    \n    with torch.no_grad():\n        for i in range(0, len(pairs), batch_size):\n            batch_pairs = pairs[i:i + batch_size]\n            inputs = tokenizer(batch_pairs, padding=True, truncation=True, \n                             return_tensors='pt', max_length=512)\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            \n            scores = model(**inputs, return_dict=True).logits.view(-1,).float()\n            all_scores.extend(scores.cpu().numpy().tolist())\n    \n    # Create result entry\n    json_entry = {\n        \"query_id\": qid,\n        \"candidates\": {\n            \"doc_ids\": doc_ids,\n            \"scores\": all_scores\n        }\n    }\n    reranked_results.append(json_entry)\n\n# Save results\nwith open('predict_top50_reranked.json', 'w') as f:\n    json.dump(reranked_results, f, indent=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T13:44:38.486095Z","iopub.execute_input":"2024-11-17T13:44:38.486860Z","iopub.status.idle":"2024-11-17T13:44:48.709818Z","shell.execute_reply.started":"2024-11-17T13:44:38.486817Z","shell.execute_reply":"2024-11-17T13:44:48.708481Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport json\n\n# Load all required data\npredictions = pd.read_json('/kaggle/input/top50cosinejson/predict_top50.json')\ntest_df = pd.read_csv('/kaggle/input/bkai-ai-track2-legal-document-retrieval/Legal Document Retrieval/public_test.csv')\ncorpus_df = pd.read_csv('/kaggle/input/preprocessed-corpus/preprocessed_corpus.csv')\n\n# # Debugging: Print first few rows to verify data format\n# print(\"First few corpus rows:\", corpus_df.head())\n# print(\"First few predictions:\", predictions.head())\n\n# Fix: Create corpus dictionary with correct column mapping\n# corpus_dict = dict(zip(corpus_df['text'], corpus_df['cid']))\n# print(\"First few corpus_dict items:\", list(corpus_dict.items())[:3])\n\n# Setup model and device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntokenizer = AutoTokenizer.from_pretrained('namdp-ptit/ViRanker')\nmodel = AutoModelForSequenceClassification.from_pretrained('namdp-ptit/ViRanker').to(device)\nmodel.eval()\n\nreranked_results = []\n\n# Process each query\nfor _, row in predictions.iterrows():\n    # Fix: Use correct key from predictions json\n    qid = row['query_id']  # or 'query_id' depending on actual json format\n    query_text = test_df['question'][test_df['qid'] == qid].iloc[0]\n    doc_ids = row['candidates']['doc_ids']\n    \n    # Debug print\n    print(f\"Processing qid: {qid}\")\n    print(f\"First few doc_ids: {doc_ids[:3]}\")\n    \n    # Create pairs with proper error handling\n    pairs = []\n    for doc_id in doc_ids:\n            # Convert doc_id to string if it's numeric\n            # doc_id_str = str(doc_id)\n            texts = corpus_df.loc[corpus_df['cid'] == int(doc_id), 'text'].iloc[0]\n            pairs.append([query_text, texts])\n    \n    # Re-rank in batches\n    batch_size = 8  # Smaller batch size due to longer texts\n    all_scores = []\n    \n    with torch.no_grad():\n        for i in range(0, len(pairs), batch_size):\n            batch_pairs = pairs[i:i + batch_size]\n            inputs = tokenizer(batch_pairs, padding=True, truncation=True, \n                             return_tensors='pt', max_length=512)\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            \n            scores = model(**inputs, return_dict=True).logits.view(-1,).float()\n            all_scores.extend(scores.cpu().numpy().tolist())\n    \n    # Create result entry\n    json_entry = {\n        \"query_id\": qid,\n        \"candidates\": {\n            \"doc_ids\": doc_ids,\n            \"scores\": all_scores\n        }\n    }\n    reranked_results.append(json_entry)\n\n# Save results\nwith open('predict_top50_reranked.json', 'w') as f:\n    json.dump(reranked_results, f, indent=2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport pandas as pd\nimport zipfile\nimport json\n\n# ...existing code...\n\n# Load JSON files using Pandas\ntrain_df = pd.read_json('/kaggle/input/encodedtrainfull/encoded_train_full.json')\ncorpus_df = pd.read_json('/kaggle/input/encoded/encoded_corpus.json')\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Extract IDs and vectors\ntrain_ids = train_df['qid'].tolist()\ntrain_vectors = torch.tensor(train_df['question_vector'].tolist(), dtype=torch.float32).to(device)\ncorpus_ids = corpus_df['cid'].tolist()\ncorpus_vectors = torch.tensor(corpus_df['vector'].tolist(), dtype=torch.float32).to(device)\n\n# Normalize vectors\ntrain_vectors = F.normalize(train_vectors, p=2, dim=1)\ncorpus_vectors = F.normalize(corpus_vectors, p=2, dim=1)\n\n# Get top K similar documents\ntop_k = 50  # Adjust as needed\n\n# Create results for both TXT and JSON formats\njson_results = []\nbatch_size = 128 # Process 8 QIDs at a time\n\nwith open('predict_top50_training.txt', 'w') as f:\n    for i in range(0, len(train_ids), batch_size): \n        # Process in batches\n        batch_train_vectors = train_vectors[i:i+batch_size]\n        batch_train_ids = train_ids[i:i+batch_size]\n        \n        # Compute cosine similarity using PyTorch on CUDA for the batch\n        similarity_matrix = torch.matmul(batch_train_vectors, corpus_vectors.T)\n        \n        # Get top K similar documents for the batch\n        top_k_values, top_k_indices = torch.topk(similarity_matrix, k=top_k, dim=1)\n        \n        for j, (indices, scores) in enumerate(zip(top_k_indices, top_k_values)):\n            qid = batch_train_ids[j] \n\n            # Convert indices and scores to Python lists\n            top_cids = [str(corpus_ids[idx.item()]) for idx in indices]\n            similarity_scores = [score.item() for score in scores]\n\n            # Write to TXT format\n            line = f\"{qid} {' '.join(top_cids)}\\n\"\n            f.write(line)\n\n            # Prepare JSON format\n            json_entry = {\n                \"query_id\": qid,\n                \"candidates\": {\n                    \"doc_ids\": top_cids,\n                    \"scores\": similarity_scores,\n                }\n            }\n            json_results.append(json_entry)\n\n# Save JSON results\nwith open('predict_top50_training.json', 'w', encoding='utf-8') as f:\n    json.dump(json_results, f, ensure_ascii=False, indent=2)\n\n# Zip both files\nwith zipfile.ZipFile('predict_top50_training.zip', 'w') as zipf:\n    zipf.write('predict_top50_training.txt')\n    zipf.write('predict_top50_training.json')\n\n# ...existing code...","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T02:38:24.993424Z","iopub.execute_input":"2024-11-18T02:38:24.993797Z","iopub.status.idle":"2024-11-18T02:43:33.554665Z","shell.execute_reply.started":"2024-11-18T02:38:24.993758Z","shell.execute_reply":"2024-11-18T02:43:33.553835Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Negative pairing for fine-tuning the cross-encoder","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport pandas as pd\nimport zipfile\nimport json\n\n# ...existing code...\n\n# Load JSON files using Pandas\ntrain_df = pd.read_json('/kaggle/input/encodedtrainfull/encoded_train_full.json')\ncorpus_df = pd.read_json('/kaggle/input/encoded/encoded_corpus.json')\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# Extract IDs and vectors\ntrain_ids = train_df['qid'].tolist()\ntrain_vectors = torch.tensor(train_df['question_vector'].tolist(), dtype=torch.float32).to(device)\ncorpus_ids = corpus_df['cid'].tolist()\ncorpus_vectors = torch.tensor(corpus_df['vector'].tolist(), dtype=torch.float32).to(device)\n\n# Normalize vectors\ntrain_vectors = F.normalize(train_vectors, p=2, dim=1)\ncorpus_vectors = F.normalize(corpus_vectors, p=2, dim=1)\n\n# Get most dissimilar document\njson_results = []\nbatch_size = 128 # Process 8 QIDs at a time\n\nwith open('predict_most_dissimilar.txt', 'w') as f:\n    for i in range(0, len(train_ids), batch_size): \n        # Process in batches\n        batch_train_vectors = train_vectors[i:i+batch_size]\n        batch_train_ids = train_ids[i:i+batch_size]\n        \n        # Compute cosine similarity using PyTorch on CUDA for the batch\n        similarity_matrix = torch.matmul(batch_train_vectors, corpus_vectors.T)\n        \n        # Get most dissimilar document (smallest similarity score)\n        min_values, min_indices = torch.min(similarity_matrix, dim=1)\n        \n        for j, (idx, score) in enumerate(zip(min_indices, min_values)):\n            qid = batch_train_ids[j]\n            cid = str(corpus_ids[idx.item()])\n            \n            # Write to TXT format\n            f.write(f\"{qid} {cid}\\n\")\n\n            # Prepare JSON format\n            json_entry = {\n                \"query_id\": qid,\n                \"candidates\": {\n                    \"doc_ids\": [cid],\n                    \"scores\": [score.item()],\n                }\n            }\n            json_results.append(json_entry)\n\n# Save JSON results\nwith open('predict_most_dissimilar.json', 'w', encoding='utf-8') as f:\n    json.dump(json_results, f, ensure_ascii=False, indent=2)\n\n# Zip both files\nwith zipfile.ZipFile('predict_most_dissimilar.zip', 'w') as zipf:\n    zipf.write('predict_most_dissimilar.txt')\n    zipf.write('predict_most_dissimilar.json')\n\n# ...existing code...","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T03:25:49.791989Z","iopub.execute_input":"2024-11-18T03:25:49.792337Z","iopub.status.idle":"2024-11-18T03:28:15.381818Z","shell.execute_reply.started":"2024-11-18T03:25:49.792304Z","shell.execute_reply":"2024-11-18T03:28:15.380992Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Fine tune with negative pairings","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset, load_dataset, concatenate_datasets\nimport os\nprint(os.listdir(\"../input\"))  # Check input directory contents (Kaggle specific)\n!pip install sentence_transformers\nimport json\nfrom sentence_transformers import (\n    SentenceTransformer,\n    SentenceTransformerTrainer,\n    SentenceTransformerTrainingArguments,\n)\nfrom sentence_transformers.losses import MultipleNegativesRankingLoss\nfrom sentence_transformers.training_args import BatchSamplers\nimport math\n\n# 1. Load a model to finetune\nmodel = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder')\n\n# 2. Load and format the dataset\ndataset = load_dataset(\"json\", data_files=\"/kaggle/input/negativepairing/fine_tune_training.json\")\ndataset = dataset[\"train\"]  # Access the main split (likely named 'train' by default)\n\n\ndef format_dataset(example):\n    return {'query': example['query'], 'positive': example['pos'], 'negative': example['neg']}\n\ndataset = dataset.map(format_dataset)\n\n# Split into train and evaluation sets (adjust split ratio as needed)\ntrain_test_valid = dataset.train_test_split(test_size=0.2, seed=42)\n\ntrain_dataset_formatted = train_test_valid[\"train\"]\ntest_dataset = train_test_valid[\"test\"]\ntrain_eval = test_dataset.train_test_split(test_size = 0.5, seed = 42)\neval_dataset_formatted = train_eval[\"train\"]\npredict_dataset_formatted = train_eval[\"test\"]\n\n\n\n# 3. Define the loss function\nloss = MultipleNegativesRankingLoss(model)\n\n# 4. Specify training arguments\nargs = SentenceTransformerTrainingArguments(\n    output_dir=\"models/vietnamese-bi-encoder-finetuned\",\n    num_train_epochs=5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    learning_rate=2e-5,\n    warmup_ratio=0.1,\n    fp16=True,  # Adjust based on your GPU\n    batch_sampler=BatchSamplers.NO_DUPLICATES,  # IMPORTANT for MultipleNegativesRankingLoss\n    evaluation_strategy=\"steps\",\n    eval_steps=100,\n    save_steps=2500,\n    save_total_limit=2,\n    logging_steps=100,\n    load_best_model_at_end=True,  # Load the best model at the end\n    metric_for_best_model=\"cos_sim\",  # Or other suitable metric\n    greater_is_better=True,  # For cosine similarity, higher is better\n    report_to=\"none\" \n)\n\n\n# 5. Create a trainer & train\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset_formatted,\n    eval_dataset=eval_dataset_formatted,\n    loss=loss\n)\ntrainer.train()\n\n\n\n# 6. Save the trained model (best model is already saved during training)\nmodel.save_pretrained(\"vietnamese-bi-encoder-finetuned/final\") # Final model (may be the same as the best, depending on settings)\n\n# Now you can evaluate on a separate test set. This should use different data than the eval set used during training.\ntrainer.evaluate(predict_dataset_formatted) # Now Evaluate on the separate test set.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T11:20:23.995313Z","iopub.execute_input":"2024-11-20T11:20:23.995692Z","iopub.status.idle":"2024-11-20T11:20:35.467193Z","shell.execute_reply.started":"2024-11-20T11:20:23.995659Z","shell.execute_reply":"2024-11-20T11:20:35.465665Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"['top50-bkai-cosine', 'encodedtrainfull', 'bkai-ai-track2-legal-document-retrieval', 'preprocessed-corpus', 'top50cosinejson', 'negativepairing', 'encoded', 'encoded-public-test']\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: sentence_transformers in /opt/conda/lib/python3.10/site-packages (3.3.1)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 72\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# 5. Create a trainer & train\u001b[39;00m\n\u001b[1;32m     65\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SentenceTransformerTrainer(\n\u001b[1;32m     66\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     67\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss\n\u001b[1;32m     71\u001b[0m )\n\u001b[0;32m---> 72\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# 6. Save the trained model (best model is already saved during training)\u001b[39;00m\n\u001b[1;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvietnamese-bi-encoder-finetuned/final\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Final model (may be the same as the best, depending on settings)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2345\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2342\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2344\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2345\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2346\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:550\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:672\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:620\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sentence_transformers/sampler.py:192\u001b[0m, in \u001b[0;36mNoDuplicatesBatchSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m batch_indices \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m remaining_indices:\n\u001b[0;32m--> 192\u001b[0m     sample_values \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    193\u001b[0m         value\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[index]\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_prompt_length\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     }\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_values \u001b[38;5;241m&\u001b[39m batch_values:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sentence_transformers/sampler.py:195\u001b[0m, in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    190\u001b[0m batch_indices \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m remaining_indices:\n\u001b[1;32m    192\u001b[0m     sample_values \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    193\u001b[0m         value\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[index]\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m--> 195\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_prompt_length\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     }\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_values \u001b[38;5;241m&\u001b[39m batch_values:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"],"ename":"TypeError","evalue":"unhashable type: 'list'","output_type":"error"}],"execution_count":4}]}